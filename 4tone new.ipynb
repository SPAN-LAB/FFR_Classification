{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720bde95-5875-430a-8fae-bf7b9dd35439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np, h5py, tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"/Volumes/gurindapalli/projects/trial_classification/4tone_cell\"\n",
    "OUT_DIR  = \"/Volumes/gurindapalli/projects/trial_classification/Jason\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== match better version defaults =====\n",
    "DS_FACTOR = 8          # downsample by 8 like your better script\n",
    "EPOCHS    = 15\n",
    "BATCH_SIZE= 32\n",
    "N_SPLITS  = 5\n",
    "VAL_SPLIT = 0.1\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdf09fa-2a75-43a5-aef8-55ef59237fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_len, n_classes=4, l2=1e-5, sdrop=0.10, head_drop=0.30):\n",
    "    inp = layers.Input(shape=(input_len,1))\n",
    "    x = layers.Conv1D(128, 9, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.SpatialDropout1D(sdrop)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(128, 9, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.SpatialDropout1D(sdrop)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 9, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.SpatialDropout1D(sdrop)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(head_drop)(x)\n",
    "\n",
    "    out = layers.Dense(\n",
    "        n_classes, activation=\"softmax\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2)\n",
    "    )(x)\n",
    "\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85af18de-b563-4422-96bd-fcd65628e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subaverage(X, y, n_classes=4, group_size=5, repeats=6, bootstrap=True, renorm=True):\n",
    "    \"\"\"\n",
    "    Build pseudo-trials by averaging 'group_size' trials of the same class.\n",
    "    Returns (X_new, y_new) or (None, None) if none created.\n",
    "    Use ONLY on TRAIN (not across splits).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "    idx_by_c = [np.where(y == c)[0] for c in range(n_classes)]\n",
    "    if any(len(idx) < group_size for idx in idx_by_c):   # if any class too small, just skip SA\n",
    "        return None, None\n",
    "    groups_per = min(len(idx)//group_size for idx in idx_by_c) * repeats\n",
    "    if groups_per <= 0:\n",
    "        return None, None\n",
    "\n",
    "    X_new, y_new = [], []\n",
    "    for c, idx in enumerate(idx_by_c):\n",
    "        for _ in range(groups_per):\n",
    "            take = rng.choice(idx, size=group_size, replace=bootstrap)\n",
    "            x_avg = X[take].mean(axis=0)  # (T,1) if X has channel\n",
    "            if renorm:\n",
    "                m = x_avg.mean(axis=0, keepdims=True)\n",
    "                s = x_avg.std(axis=0, keepdims=True) + 1e-7\n",
    "                x_avg = (x_avg - m) / s\n",
    "            X_new.append(x_avg); y_new.append(c)\n",
    "    return np.stack(X_new, axis=0), np.array(y_new, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d7a9c8-cb83-49b2-9375-5da13fe968be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels_like_better(f):\n",
    "    lab = f[\"labels\"]                                     # cell array of refs (1 x N)\n",
    "    raw = np.array([int(np.array(f[lab[0, i]])[0, 0])     # deref each cell to scalar\n",
    "                    for i in range(lab.shape[1])])\n",
    "    u = np.sort(np.unique(raw))\n",
    "    tone_to_idx = {v:i for i, v in enumerate(u)}          # per-file rank remap → 0..3\n",
    "    y = np.array([tone_to_idx[v] for v in raw], dtype=np.int32)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334ce19f-a51e-43fa-9efd-35a91fc7b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_like_better(mat_path, dataset=\"ffr_dss\", ds_factor=8):\n",
    "    with h5py.File(mat_path, \"r\") as f:\n",
    "        X = f[dataset][()]                 # expected (N, 4997); transpose if needed\n",
    "        if X.shape[0] == 4997 and X.ndim == 2:\n",
    "            X = X.T\n",
    "        y = decode_labels_like_better(f)\n",
    "\n",
    "    # downsample like better script\n",
    "    if ds_factor is not None and ds_factor > 1:\n",
    "        X = X[:, ::ds_factor]\n",
    "\n",
    "    # per-trial z-norm across time, then add channel dim\n",
    "    m = X.mean(axis=1, keepdims=True); s = X.std(axis=1, keepdims=True) + 1e-7\n",
    "    X = ((X - m) / s)[..., np.newaxis]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78026f3-a386-40f5-9187-d641b3ed7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_on_file(MAT_PATH):\n",
    "    X, y = load_like_better(MAT_PATH, dataset=\"ffr_dss\", ds_factor=DS_FACTOR)\n",
    "    print(f\"{os.path.basename(MAT_PATH)} -> X={X.shape}, y={y.shape}, uniques={np.unique(y)}\")\n",
    "    print(\"class counts:\", np.bincount(y, minlength=n_classes).tolist())\n",
    "\n",
    "    counts = np.bincount(y, minlength=n_classes)\n",
    "    if (counts < N_SPLITS).any():\n",
    "        raise ValueError(f\"{os.path.basename(MAT_PATH)}: some class has < {N_SPLITS} samples: {counts.tolist()}\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True)  # same as better version\n",
    "    fold_accs = []\n",
    "    cm_total  = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for i, (tr, te) in enumerate(skf.split(X, y), 1):\n",
    "        # explicit stratified train/val from training fold (same as better version)\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SPLIT)\n",
    "        tr_sub_idx, val_idx = next(sss.split(X[tr], y[tr]))\n",
    "        Xtr, ytr = X[tr][tr_sub_idx], y[tr][tr_sub_idx]\n",
    "        Xval, yval = X[tr][val_idx],   y[tr][val_idx]\n",
    "        Xte,  yte  = X[te],            y[te]\n",
    "\n",
    "        # Train-only sub-averages (same as better version)\n",
    "        Xtr_sa, ytr_sa = subaverage(Xtr, ytr, n_classes, group_size=5, repeats=6, bootstrap=True, renorm=True)\n",
    "        if Xtr_sa is not None:\n",
    "            Xtr_aug = np.concatenate([Xtr, Xtr_sa], axis=0)\n",
    "            ytr_aug = np.concatenate([ytr, ytr_sa], axis=0)\n",
    "        else:\n",
    "            Xtr_aug, ytr_aug = Xtr, ytr\n",
    "\n",
    "        # Build model (same as better version)\n",
    "        model = build_model(X.shape[1], n_classes)\n",
    "\n",
    "        # Bias-init to priors of augmented train set (same as better version)\n",
    "        priors = np.bincount(ytr_aug, minlength=n_classes) / len(ytr_aug)\n",
    "        last = model.layers[-1]\n",
    "        if hasattr(last, \"bias\") and last.bias is not None:\n",
    "            last.bias.assign(np.log(priors + 1e-8))\n",
    "\n",
    "        # Simple class-weights from augmented counts (same as better version)\n",
    "        class_counts = np.bincount(ytr_aug, minlength=n_classes).astype(np.float32)\n",
    "        class_weight = {k: (len(ytr_aug) / (n_classes * max(float(c), 1.0))) for k, c in enumerate(class_counts)}\n",
    "\n",
    "        es  = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
    "        rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5)\n",
    "\n",
    "        print(f\"\\n{os.path.basename(MAT_PATH)} | Fold {i}: counts  \"\n",
    "              f\"train={np.bincount(ytr, minlength=n_classes).tolist()}  \"\n",
    "              f\"val={np.bincount(yval, minlength=n_classes).tolist()}  \"\n",
    "              f\"test={np.bincount(yte,  minlength=n_classes).tolist()}  \"\n",
    "              f\"priors={np.round(priors,3).tolist()}\")\n",
    "\n",
    "        model.fit(\n",
    "            Xtr_aug, ytr_aug,\n",
    "            validation_data=(Xval, yval),   # val is single-trial (no SA), same as better version\n",
    "            epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,\n",
    "            callbacks=[es, rlr],\n",
    "            class_weight=class_weight\n",
    "        )\n",
    "\n",
    "        # Evaluate on held-out test fold (same as better version)\n",
    "        y_hat = np.argmax(model.predict(Xte, batch_size=BATCH_SIZE, verbose=0), axis=1)\n",
    "        acc   = accuracy_score(yte, y_hat)\n",
    "        fold_accs.append(acc)\n",
    "        cm_total += confusion_matrix(yte, y_hat, labels=list(range(n_classes)))\n",
    "\n",
    "        # Debug: prediction usage per fold\n",
    "        pred_counts = np.bincount(y_hat, minlength=n_classes)\n",
    "        print(f\"{os.path.basename(MAT_PATH)} | Fold {i}: acc={acc:.4f} | pred counts={pred_counts.tolist()}\")\n",
    "\n",
    "    print(f\"{os.path.basename(MAT_PATH)} | Mean acc: {np.mean(fold_accs):.4f} ± {np.std(fold_accs):.4f}\")\n",
    "    return fold_accs, cm_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9276f3-6633-4a39-910d-2404b9f753b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Unable to synchronously open file (unable to lock file, errno = 13, error message = 'Permission denied')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmat_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (skipping)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m fold_accs, cm_total \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cv_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4T\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m accs_by_file[key] \u001b[38;5;241m=\u001b[39m fold_accs\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mrun_cv_on_file\u001b[0;34m(MAT_PATH)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_cv_on_file\u001b[39m(MAT_PATH):\n\u001b[0;32m----> 2\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mload_like_better\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMAT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mffr_dss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDS_FACTOR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(MAT_PATH)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> X=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, uniques=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39munique(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass counts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mbincount(y, minlength\u001b[38;5;241m=\u001b[39mn_classes)\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36mload_like_better\u001b[0;34m(mat_path, dataset, ds_factor)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_like_better\u001b[39m(mat_path, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffr_dss\u001b[39m\u001b[38;5;124m\"\u001b[39m, ds_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m         X \u001b[38;5;241m=\u001b[39m f[dataset][()]                 \u001b[38;5;66;03m# expected (N, 4997); transpose if needed\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4997\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Unable to synchronously open file (unable to lock file, errno = 13, error message = 'Permission denied')"
     ]
    }
   ],
   "source": [
    "file_ids = list(range(1002, 1016))  # 1002..1015 inclusive\n",
    "accs_by_file = {}                   # { \"4T1002\": [fold accs], ... }\n",
    "labels = []\n",
    "agg_rows = []\n",
    "\n",
    "for fid in file_ids:\n",
    "    fname = f\"4T{fid}.mat\"\n",
    "    mat_path = os.path.join(BASE_DIR, fname)\n",
    "    if not os.path.exists(mat_path):\n",
    "        print(f\"Missing file: {mat_path} (skipping)\")\n",
    "        continue\n",
    "\n",
    "    fold_accs, cm_total = run_cv_on_file(mat_path)\n",
    "    key = f\"4T{fid}\"\n",
    "    accs_by_file[key] = fold_accs\n",
    "    labels.append(key)\n",
    "\n",
    "    row = {\"file\": key}\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            row[f\"cm_{i}{j}\"] = int(cm_total[i, j])\n",
    "    agg_rows.append(row)\n",
    "\n",
    "    # free up memory between runs\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "agg_cols = [\"file\"] + [f\"cm_{i}{j}\" for i in range(n_classes) for j in range(n_classes)]\n",
    "df_agg = pd.DataFrame(agg_rows, columns=agg_cols)\n",
    "agg_csv_path = os.path.join(OUT_DIR, \"cm_all_files.csv\")\n",
    "df_agg.to_csv(agg_csv_path, index=False)\n",
    "print(f\"\\nSaved aggregated confusion matrices → {agg_csv_path}\")\n",
    "\n",
    "print(\"\\nPer-file mean ± SD across CV folds:\")\n",
    "for k in labels:\n",
    "    m = float(np.mean(accs_by_file[k]))\n",
    "    sd = float(np.std(accs_by_file[k]))\n",
    "    print(f\"{k}: {m:.4f} ± {sd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eac878-d650-4424-b43c-30de17f360a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
